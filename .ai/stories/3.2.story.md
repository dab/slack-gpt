# Story 3.2: Implement `/ask` Command Handler - Core Flow & Orchestration

## Story

**As a** Developer\
**I want** to implement the core logic and service orchestration for the `/ask` slash command handler\
**so that** the bot can process user questions, check the cache, query the knowledge base and OpenAI, cache the result, and send an asynchronous response.

## Status

Approved

## Context

This is the main story implementing the primary functionality (PRD FR#1). It involves creating the asynchronous handler in `app/handlers/ask_command.py`, registering it, and orchestrating calls to the services created in Epic 2 (`KnowledgeBaseService`, `RedisService`, `OpenAIService`). Key aspects include immediate acknowledgment (`ack()`), asynchronous execution of the main logic, question parsing/normalization, cache key generation (SHA-256 hash as per Arch Doc ADR-004), cache check/hit, context retrieval, OpenAI call (if cache miss), caching the new result, and sending the final answer asynchronously using Block Kit (`respond`). Error handling is covered separately in Story 3.3.

## Estimation

Story Points: 5

## Acceptance Criteria

1.  - [x] Create `app/handlers/ask_command.py`.
2.  - [x] Define `async def handle_ask_command(ack, command, respond, logger, ...)` signature. Inject service instances if using dependency injection, or instantiate them.
3.  - [x] Add imports for services, `hashlib`, `re` (for normalization), etc.
4.  - [x] Implement immediate `await ack()`.
5.  - [x] Extract question text: `question = command.get('text', '').strip()`.
6.  - [x] Implement check for empty question and send usage message via `await respond(...)` if empty.
7.  - [x] Implement normalization function (e.g., `lower()`, `re.sub(...)`).
8.  - [x] Implement cache key generation: `hashlib.sha256(normalized_question.encode()).hexdigest()`.
9.  - [x] Implement cache check logic: `cached_answer = await redis_service.get_value(cache_key)`.
10. - [x] Implement cache hit path: format `cached_answer` with Block Kit and `await respond(...)`.
11. - [x] Implement cache miss path:
    *   - [x] `context = await knowledge_base_service.find_relevant_context(question)`.
    *   - [x] `answer = await openai_service.get_answer(question, context)`.
12. - [x] Implement caching result: `if answer: await redis_service.set_value(cache_key, answer, ttl=86400)`.
13. - [x] Implement sending OpenAI result: `if answer: await respond(blocks=format_block_kit(answer))`.
14. - [x] Define `format_block_kit(answer_text)` helper function.
15. - [x] Register handler in `app/main.py`.
16. - [x] Manually test the `/ask` command flow (cache miss, then cache hit).

## Testing Requirements:

*   Code coverage requirement >= 70% for `app/services/`. (Handler testing with mocked services recommended post-MVP).

## Story Wrap Up (To be filled in AFTER agent execution):

*   **Agent Model Used:** GPT-4.1
*   **Agent Credit or Cost:** N/A (local dev)
*   **Date/Time Completed:** 2025-04-23 12:11:04 UTC
*   **Commit Hash:** 0b69afa409735ba8cf0001d1016b31c174334ccb
*   **Change Log**
    *   Implemented async /ask command handler with cache, KB, and OpenAI integration
    *   Registered handler in main.py with async Slack Bolt app
    *   Added aiohttp and switched to AsyncOAuthSettings for compatibility
    *   Manual Slack test verified (cache miss and hit)
    *   Error handling and logging improved 