# Story 5.1: Refactor Knowledge Base Context Handling with Token Limits

## Story

**As a** Developer
**I want** to refactor the `KnowledgeBaseService.find_relevant_context` method
**so that** it selects relevant PDF text chunks based on an estimated token count to fit within the OpenAI model's context window, rather than using a character limit.

## Status

Draft

## Context

Currently, `find_relevant_context` aggregates all relevant text chunks found via keyword search and then truncates the result based on a `max_chars` limit. This approach doesn't account for OpenAI's token limits and can lead to prompts that are too large for the API or don't effectively utilize the available context window. This story refactors the logic to use token estimation for better control.

## Estimation

Story Points: 5 (1 SP=1 day of Human Development, or 10 minutes of AI development)

## Acceptance Criteria

1.  - [x] `tiktoken` library added to `requirements.txt`.
2.  - [x] `KnowledgeBaseService.find_relevant_context` updated:
    *   - [x] No longer uses the `max_chars` parameter for final truncation.
    *   - [x] Accepts a `max_context_tokens` parameter (defaulting to a reasonable value for `gpt-4o-mini`, e.g., 7000, configurable via `config.py`).
    *   - [x] Uses `tiktoken` (with the appropriate encoding for `gpt-4o-mini`) to estimate the token count of each relevant text chunk found.
    *   - [x] Implements a strategy to select the most relevant chunks (e.g., based on keyword density or simple ordering) until the total estimated token count approaches `max_context_tokens`.
    *   - [x] Ensures the final concatenated context string respects the token limit.
3.  - [x] `app/utils/config.py` includes a new configuration variable `MAX_CONTEXT_TOKENS` loaded from an environment variable, with a sensible default. `.env.example` updated accordingly.
4.  - [x] Existing unit tests in `tests/services/test_knowledge_base.py` updated to reflect the new token-based logic (mocking `tiktoken` might be necessary). Test cases should cover scenarios where context is truncated due to token limits.
5.  - [x] Logging updated to indicate when context is being selected/truncated based on token limits.
6.  - [x] `README.md` updated to mention `tiktoken` dependency and the `MAX_CONTEXT_TOKENS` environment variable.
7.  - [x] `arch.md` updated to reflect the use of `tiktoken`, token-based context selection in `KnowledgeBaseService`, and the new `MAX_CONTEXT_TOKENS` configuration variable.

## Subtasks

1.  - [x] **Dependencies & Configuration**
    1. - [x] Add `tiktoken` to `requirements.txt`.
    2. - [x] Add `MAX_CONTEXT_TOKENS` configuration in `config.py` and `.env.example`.
    3. - [x] Research the appropriate `tiktoken` encoding for `gpt-4o-mini`.
2.  - [x] **Refactor Knowledge Base Service**
    1. - [x] Integrate `tiktoken` for estimation in `find_relevant_context`.
    2. - [x] Implement context selection logic based on token budget.
    3. - [x] Remove/adapt `max_chars` logic in `find_relevant_context`.
    4. - [x] Update logging messages related to context handling.
3.  - [x] **Testing**
    1. - [x] Update unit tests in `test_knowledge_base.py` for token logic.
4.  - [x] **Documentation**
    1. - [x] Update `README.md`.
    2. - [x] Update `arch.md`.
5.  - [x] **Code Quality**
    1. - [x] Run linters/formatters/type checkers.

## Testing Requirements:

*   Unit tests must mock `tiktoken` and test various scenarios of context selection and truncation based on token limits.
*   Unit tests for `app/services/knowledge_base.py` must maintain or exceed >= 70% code coverage.

## Story Wrap Up (To be filled in AFTER agent execution):

*   **Agent Model Used:** GPT-4.1
*   **Agent Credit or Cost:** N/A (local dev)
*   **Date/Time Completed:** 2025-04-23 18:43 EEST
*   **Commit Hash:** <To be filled after commit>
*   **Change Log**
    *   Added tiktoken to requirements and documented in README/arch.md
    *   Refactored KnowledgeBaseService to use token-based context selection
    *   Updated config.py for MAX_CONTEXT_TOKENS
    *   Updated and extended unit tests for token logic
    *   Improved logging for context truncation 